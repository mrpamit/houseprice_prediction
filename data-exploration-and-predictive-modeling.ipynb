{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## PREDICTING HOUSE PRICES USING ADVANCED REGRESSION","metadata":{"_uuid":"491f434f5b3bcd7051683ea264d96afb4695a0ed"}},{"cell_type":"markdown","source":"### PROBLEM STATEMENT","metadata":{"_uuid":"b8e45f8ca18f435428d161ff4b9344e4e7ffc835"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">The problem will be addressed if we build predictive models using Advanced Regression Techniques and train the model and pick the best model using evaluators so that it can accurately predict the value of House price which is **SalePrice** - Target variable.</span>","metadata":{"_uuid":"575f6a1e71c549d4e61e69d04e37375330910d5b"}},{"cell_type":"markdown","source":"### CLIENT","metadata":{"_uuid":"911ceb33a18d100dd6b896242fec7665f48de802"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Our Fictional Client is one of the **House Brokerage Firms** in Ames, Iowa looking to determine the exact house price by using our model predictions. Well! it's very important to satisfy the client requirements and hence the best model should be built to predict house prices accurately. Let's dig in!</span>","metadata":{"_uuid":"d91d7e5f0682b0f4e1af8a7973dd53164fbf2192"}},{"cell_type":"markdown","source":"### ABOUT THE DATASET","metadata":{"_uuid":"b76e7effff2a8ac2514167480b48b52c18f8cee2"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">The dataset is obtained from Kaggle and it is a Competition Dataset which contains **79 features** which influence the price of Homes at Ames, Iowa. The competition organizer also boasts about the advantages of not just estimating house price using No. of bedrooms or the fence around the house usually done by the brokers. Yeah, he is right, when you can accurately predict the house price using **Advanced Regression** techniques then why bother about a House Broker's price estimate?</span>","metadata":{"_uuid":"e29d5acf2990856726780ad9f45f92370b48dd74"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\"> We need a variety of pre-processing steps to deal with before dealing with the Big Picture (i.e) **Finding the Best Model**.\nThe below steps make sure that we are on track to achieve the prime target i.e Predicting the Sale\nPrice </span>","metadata":{"_uuid":"8aca97b3daeed6ac453405a652743f1fbe060310"}},{"cell_type":"markdown","source":"1. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Data Cleaning, Outlier Analysis and Exploratory Data Analysis</span>\n2. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Use Heatmap and Correlation to find the Correlation among features</span>\n3. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Analyze all the features and perform log transform for the skewed features</span>\n4. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Data Visualization</span>\n5. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Dummy the Categories and Finding Most Important Features</span>\n6. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Model building including Lasso and Ridge Regression</span>\n7. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Model Validation using plenty of good validators</span>\n8. <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Conclusion by publishing the research, findings and result\n</span>","metadata":{"_uuid":"42b0f631e9413eea6ef3c66335b4ca1c121308bd"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">\n> “**People who work hard and people who work smart have different measures of success.”**-\n*Jacob Morgan*</span>\n<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">\nIt's all about the smart work we do to make our work easy. People say Mathematicians are usually lazy and find an easy way to solve a hard complex mathematical problem.Let's start doing the Smart Work!!</span>","metadata":{"_uuid":"7cdff2aca35d42ad0cf459cd678f160c0cd31045"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">As usual we import all our basic libraries, scipy and \nplenty of model elements from scikit learn</span>","metadata":{"_uuid":"ac0ef834440525e2e65bd6820d312ea7fefa8c0d"}},{"cell_type":"code","source":"# Importing DataScience libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm,skew\nfrom scipy import stats\n%matplotlib inline\nfrom sklearn.linear_model import LinearRegression,Ridge,RidgeCV, ElasticNetCV, LassoCV,BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler,StandardScaler,MinMaxScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error","metadata":{"_uuid":"d0556850e083c22f1bc9659e95ceea54175af29a","collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Lets's load the Train and Test/ Holdout set into a Pandas Dataframe. The key here is not to touch the Test/ Holdout set till the last and work with only the Train set</span>","metadata":{"_uuid":"dafbedbfb5b6b53cd932a7b62357846420c08f9d"}},{"cell_type":"code","source":"# Reading csv file and making'Id' as Index\ndf_train = pd.read_csv('../input/train.csv',index_col = 'Id')\ndf_test = pd.read_csv('../input/test.csv',index_col = 'Id')","metadata":{"_uuid":"f5c737eb653498353d2b7fb9c039931d719ec404","trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Let's look at the top few rows\ndf_train.head()","metadata":{"_uuid":"5520c02dae1ee01e6ecb11db6141ccb36a814eb2"},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# The rows and columns of our dataset \ndf_train.shape","metadata":{"_uuid":"b6d04384fa3b80d909731b8293e1d5a6fe5b79b0"},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Well we have to deal with plenty of attributes \ndf_train.columns","metadata":{"_uuid":"e8e8c3f785a61ade04af9b6d005439ad6970734f"},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Datatype of each attribute\ndf_train.dtypes","metadata":{"_uuid":"b7645f2ecfadb4de3d0c5b04d91712490b6768ce"},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Descriptive Statistics of Numerical Variables\ndf_train.describe()","metadata":{"_uuid":"0127e135607af4549ab49671fdedaf5de8a13c8c"},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Statistics of our Categorical variables\ndf_train.describe(include=['O'])","metadata":{"_uuid":"a32a5debfe99b05abdd56419ac2f3272518b1c9c"},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### MISSING DATA","metadata":{"_uuid":"61b63c5abf2147d178b45d436967c1972b51bf83"}},{"cell_type":"markdown","source":"\n* <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Missing data is always a concern, you wonder every time what happens to the missing values!</span>\n\n* <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Whether it was not entered during Data entry? </span> \n\n* <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Whether it got missed during Data Imports/Exports? </span> \n\n* <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Whatever maybe the reason we still can impute them.</span>\n\n* <span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">First we check for the percentage of missing values in each fields and work each one separately.</span>","metadata":{"_uuid":"dbfb98829b2755610025432a53f8dea597470063"}},{"cell_type":"code","source":"# Checking for the Missing values\n# Using isnull fuction to count the total null values in each field\ntotal = df_train.isnull().sum().sort_values(ascending=False) \n# Percent of missing values is estimated by dividing total missing and the original total\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n# Concatenating the Total and Percent fields sing pandas concat fucntion\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Displays top 20 from our max sorted list\nmissing_data.head(20)","metadata":{"_uuid":"706839d04a6c4a09bbe310148d4cca71c9ebf9a2"},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Inserting \"None\" for the missing values for fields \n\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\" and \"FireplaceQu\" since they are entered as NA which means its not present</span>","metadata":{"_uuid":"8ea15f5f4e7d4e505f938044b77c0dedd4d3c47b"}},{"cell_type":"code","source":"# Pandas fillna method to fill missing values with None\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence','FireplaceQu'):\n    df_train[col] = df_train[col].fillna('None')","metadata":{"_uuid":"a9126ce6f873c586e654386d0b0a03952287fe3e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">For \"LotFrontage\", the area connected to the house is almost similar when compared to the neighborhood and hence we fill the median for missing values</span>","metadata":{"_uuid":"a3afb986b4403d9e208e249c9ca29078d43d87d3"}},{"cell_type":"code","source":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ndf_train[\"LotFrontage\"] = df_train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","metadata":{"_uuid":"cb569dde7ec40b637f49bd0ca0aa78c11b522d6a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# For loop to replace the missing data in the 4 attributes to None \nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    df_train[col] = df_train[col].fillna('None')","metadata":{"_uuid":"730c10594499fe994bfd34c8b983d1904a4495e9","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Filling 0 for GarageYrBlt since it depicts No Garage for missing values\ndf_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)","metadata":{"_uuid":"72d6f053656cc09c9873aabba8914cfe8f9bd10a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Filling with None since missing values means there is no Basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    df_train[col] = df_train[col].fillna('None')","metadata":{"_uuid":"aed6b3c90167e6a90726ae5c2bf1e0f4d32c7bcb","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Filling *0* for Masonary Veener Area because its NA and  Type as *None* meaning not present for these houses</span>","metadata":{"_uuid":"35a7789894d77ef735d5fee61ac17ba2fea92817"}},{"cell_type":"code","source":"df_train[\"MasVnrType\"] = df_train[\"MasVnrType\"].fillna(\"None\")\ndf_train[\"MasVnrArea\"] = df_train[\"MasVnrArea\"].fillna(0)","metadata":{"_uuid":"5fde7480e0e39abd9c54a515f71905b2afe3fd8c","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Since the default functionality is **Typical** as per the metadata we can impute the missing values to be *Typ* </span>","metadata":{"_uuid":"a8d5461e1d4915343ba7629316dd162c9f69cf7b"}},{"cell_type":"code","source":"df_train[\"Functional\"] = df_train[\"Functional\"].fillna(\"Typ\")","metadata":{"_uuid":"51d62a5da93ddabd3b7c9bc0bcb5e35874c4aef0","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Imputing the most occuring field for Electrical since mostly all houses have Electricity\ndf_train['Electrical'] = df_train['Electrical'].fillna(df_train['Electrical'].mode()[0])","metadata":{"_uuid":"ff2e957f1d7e3735095fb90575dd3043d785e6b5","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\"> Double checking for missing values. Well, looks\nlike we are clean!!</span>","metadata":{"_uuid":"3d187f0c3113999fb9aea515f572e6d67ba4af4f"}},{"cell_type":"code","source":"# Well! Things look better now! \ndf_train.isnull().sum().max()","metadata":{"scrolled":true,"_uuid":"9e2f869fa664d535180818a64b7fe6f3c5fe3f8e"},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### OUTLIER ANALYSIS","metadata":{"_uuid":"0ca935447ec047cf7916901d6171404ef96bd1c9"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">To deal with Outlier is difficult since it varies with each attribute For example outliers are much used in Credit Card Fraud since they can show unusual patterns in the spending activity, eliminating them there is a really big blunder whereas in other areas such as our case it doesn't make much sense since it makes our analysis difficult so in other cases such as ours its better to eliminate such unusual activity. Boxplot helps us a lot in finding Outliers in our case we use a Scatterplot to compare the SalePrice with Living Area</span>","metadata":{"_uuid":"1a7d66586cb8f5ac22a127d214dbde99e6bffb68"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Using Scatter plot and performing Bivariate analysis helps in finding outliers in our data. Below clearly there are two data points in GrLivArea which can be eliminated since for a Larger Area the SalePrice is unbelievably less!</span>","metadata":{"_uuid":"81923f6d0df71a87ea61f22cc16a13c9bcbe1763"}},{"cell_type":"code","source":"#Scatterplot of Area(sq ft) and SalePrice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\n# set ylimit to limit our data\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,1000000));","metadata":{"_uuid":"111c7eeb5d405b903d216eb84a430173cb9e04f2"},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Sorted descending to pick the values of GrLivArea for them to drop it\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train = df_train.drop(df_train[df_train['GrLivArea'] == 5642].index)\ndf_train = df_train.drop(df_train[df_train['GrLivArea'] == 4676].index)","metadata":{"_uuid":"4647650f85001ba7514592ab1c0eaa4759d191fe","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">The plot below doesn't have our eliminated values, all \ngood so far :) </span>","metadata":{"_uuid":"3165f42d12bde45be131ca6b3d930fad5c4cb041"}},{"cell_type":"code","source":"# Rechecking the plot after outlier elimination\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,1000000));","metadata":{"_uuid":"6ea68d88b18283a376145124a053bfaa71f9ab14"},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Total sq feet of Basement area and SalePrice comparison\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","metadata":{"scrolled":true,"_uuid":"5c5741be6c45bdb582b18aad975325f3a7fc51b5"},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Let's pick few features of our interest and apply them in a ParGrid to see what's going on! Well, SalesPrice has a strong positive relationship with the **OverallQual**,**GarageCars**,**FullBath**,**SalePrice** though there are some differences in space for Garage cars as the SalePrice goes down after 3 garages, that's interesting!</span>","metadata":{"_uuid":"1a887bebba85432c30f074740def1be4774c6c03"}},{"cell_type":"code","source":"#Let's look at some more variables which affect/influence the Target variable\ng = sns.PairGrid(df_train,\n                 x_vars=['OverallQual','GarageCars','FullBath'],\n                 y_vars=[\"SalePrice\"],\n                 aspect=.75, size=6)\n#plt.xticks(rotation=90)\ng.map(sns.barplot, palette=\"coolwarm\");","metadata":{"_uuid":"a565c31db99d32ff432ea2a0b69328aa58269e46"},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">**Pairplot** from Seaborn is a cool visualization to use since it brings all our important features of consideration in a single\nplot, below we can see very nice positive correlations between Sales Price, Living Area, TotalBsmtSF and YearBuilt</span>","metadata":{"_uuid":"a257548771b0e97bfbbebce773c96f75da63a8eb"}},{"cell_type":"code","source":"# seaborn pairplot\nsns.set(style= 'ticks')\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","metadata":{"_uuid":"763a6f7dc3dde321914d10d9776fd3b05a0b2686"},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"### OUR TARGET : SALEPRICE","metadata":{"_uuid":"ea7fa7fcd013001102af991c0dabaa9a4fa53cd7"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Our **Dependent Target Variable SalePrice** is plotted in a Histogram and it doesnt follow normal distribution but positively skewed\nand the curve shows most SalePrice lies within \\$150000-\\$200000</span>","metadata":{"_uuid":"83841c87d531870a0ce90726a79f916d899e1303"}},{"cell_type":"code","source":"# Using Seaborn to create a distplot with 20 bins\nsns.distplot(df_train['SalePrice'],bins = 20);","metadata":{"_uuid":"5cbfe450f60dd526ef8495d5c937898493f63031"},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Let's address our skewed target and normalize it using numpy log transformation!</span>","metadata":{"_uuid":"98f990ce1b4e4f3e4d5b5efb6993f322a95e4bf2"}},{"cell_type":"code","source":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","metadata":{"scrolled":true,"_uuid":"8436072e80b503a1505ca4903aff58232bc0d4ba"},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">We have fixed the skewness problem now and the Target variable is normally distributed after the Log transformation!</span>","metadata":{"_uuid":"f919d7dc2626ba3c5d18639dea8323b235be6fb9"}},{"cell_type":"markdown","source":"### CORRELATION & HEATMAP","metadata":{"_uuid":"a60b8027729fddc8fb8bc927c48ea56d853a1f01"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">A lot is happening with the Correlation heatmap below.Most important features determined by the positive Correlation score and OverallQuality seems to be the leader, obvious though!</span>","metadata":{"_uuid":"809613a7348c1c3b3fbda3c7428832201fbbb1d3"}},{"cell_type":"code","source":"sns.set(style=\"white\")\ncorr = df_train.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(20,20))\ncmap = sns.diverging_palette(1000, 50, as_cmap=True)\nsns.heatmap(corr,annot=True, mask=mask,vmax =.8, cmap=cmap, center=0,fmt= '.2f',\n            square=True, linewidths=.1, cbar_kws={\"shrink\": .5});","metadata":{"_uuid":"a4504f4e2cf1154fe2dff65eedc2bc2dbedb781b"},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"### FEATURE ENGINEERING","metadata":{"_uuid":"6f3414e8f5399ad4c061161ba4c983517477cb56"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">To get our features right and normally distributed solves most of the problem. Well, skewness is a issue to address here and the numeric features are filtered and applied the log transformation</span>","metadata":{"_uuid":"20365da2fb2f780f9803a0194728b4715638f84a"}},{"cell_type":"code","source":"# Using datatype to seperate numerical features\nnumeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n\n# Computing skewness using lambda function\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x.dropna()))\n# Filtering highly skewed features\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index","metadata":{"_uuid":"072ed2728b710301aab613e88ac0dd0e82d3d604","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#Log transform to normalize skewed features\ndf_train[skewed_feats] = np.log1p(df_train[skewed_feats])","metadata":{"_uuid":"55934388a3adb62596165577a9a5f9275f9b007f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">**Getting dummies for categorical variables**</span>","metadata":{"_uuid":"12f39604798a77dfd588fa9f47b1a18f3ec3162c"}},{"cell_type":"code","source":"# pandas get_dummies function to convert categorical values into binary\ndf_train = pd.get_dummies(df_train)","metadata":{"_uuid":"a6ea89425f5c09633d7cf69d4e9f7921927c9e4e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">**Formulating *X* and *y* arrays**</span>","metadata":{"_uuid":"f8d0e4f901bc46f6ca2bbddb8fd1c10c25742f3b"}},{"cell_type":"code","source":"# X array - drop target\nX= df_train.drop(['SalePrice'],axis=1).values","metadata":{"_uuid":"defad6db294caf8005298d71eeba509acaed6d40","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# y array - target only\ny=df_train['SalePrice'].values","metadata":{"_uuid":"8e591f34c6751d7b39b2415aa9d3cebe9116b0fa","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">** Making a Train Validate Split from Train**</span>","metadata":{"_uuid":"bc47c821389c4c2f54225b74d5d8b40f20692183"}},{"cell_type":"code","source":"# Split Train into Train and Validate Sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20,random_state=42)","metadata":{"_uuid":"09e1a91b3906bdb37fa27c5e15071dd83b952511","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Size of our newly split datasets \nprint('Train size: %i' % X_train.shape[0])\nprint('Validation size: %i' % X_val.shape[0])","metadata":{"_uuid":"e7d4b1c7efceb2b8f38e92652fcf4457810a6487","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">**Function to return RMSE (Root Mean Squared Error)of our K-Fold Cross Validation**</span>","metadata":{"_uuid":"380a44930185d9c38999b431e60c95b72150e7a0"}},{"cell_type":"code","source":"# No. of folds for Cross Validation\nn_folds = 5\n# Defining a function which returns the mean squared error between the cross validation score of X and y arrays\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42)\n    rmse= np.sqrt(-cross_val_score(model,X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","metadata":{"_uuid":"2c07e92f9a06d3c0ff417ebe1d6c1bdc5274833b","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"#### CROSS VALIDATION","metadata":{"_uuid":"38458215753306eaa6346d174230ebaaf87d5c47"}},{"cell_type":"code","source":"# Intialize the Cross Validation function with Shuffle\nn_folds = 5\ncustom_cv = KFold(n_folds, shuffle=True, random_state=42)","metadata":{"_uuid":"d93525f1729515a5d55eb089e9838a2532be3291","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"### PREDICITIVE MODELS","metadata":{"_uuid":"b850b9b8d66e3817629292176c05cfda118b120c"}},{"cell_type":"markdown","source":"#### BASE MODEL : GRADIENT BOOSTING REGRESSOR","metadata":{"_uuid":"178d9cae33a9a88e1db652b583e8640f53ffac3b"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Let's start with the Gradient Boosting Regressor as our Base Model.GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.</span>","metadata":{"_uuid":"fb96e6e9052456cfdbb5d54009ab542a50116de0"}},{"cell_type":"code","source":"#define  our  GB model\nGBR = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","metadata":{"_uuid":"45dc549c82747e49b4965dd074a1e5eb34943c53","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"#fitting he model on the train set\nGBR.fit(X_train,y_train)","metadata":{"_uuid":"759f7eeb3572c1dd3c839a36ac796a1bb48d1a19"},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Cross Validation Error\nrmse_cv(GBR).mean()","metadata":{"_uuid":"1ed57293f8751f8a16ae2103ff985741d5d487b1"},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# R squared, RMSE and Mean Absolute Error obtained by using actual and predicted values from Validate set\nprint('Validation R^2: %.5f'  % r2_score(y_val,GBR.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,GBR.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,GBR.predict(X_val)))","metadata":{"_uuid":"3055db070d7549c375d938f9a9cecf512837942e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**RIDGE WITH CV**</span>","metadata":{"_uuid":"def68d6995d82523057f34d99503a6c6fcb27537"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Ridge regression with built-in cross-validation.\nBy default, it performs Generalized Cross-Validation, which is a form of efficient Leave-One-Out cross-validation.</span>","metadata":{"_uuid":"c5b22f2be5ef42f01522798adbfee80d7e1892bd"}},{"cell_type":"code","source":"# ridgecv model\nridgecv = RidgeCV(alphas=[0.1, 1.0, 10.0],cv =custom_cv)","metadata":{"_uuid":"a0824e70fc07c5fe2354e3139a5a6deae0306594","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#fitting our model on the training data\nridgecv.fit(X_train,y_train)","metadata":{"_uuid":"a311d206e3a3cdbcc1938100151ae6ad1938a3c0"},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# let's call our function to calculate the mean cross validation error\nrmse_cv(ridgecv).mean()","metadata":{"_uuid":"dc40983048da0d962be169a652b3fc911248f2ad"},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"# R- squared, RMSE and Mean Absolute Error for the validation sets\nprint('Validation R^2: %.5f'  % r2_score(y_val,ridgecv.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,ridgecv.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,ridgecv.predict(X_val)))","metadata":{"_uuid":"458396ea18ecd54923c53c8e5fd9267a04262cbd","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**KERNEL RIDGE**</span>","metadata":{"_uuid":"dd839504740360f64441c02f5783928595b9760e"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.</span>","metadata":{"_uuid":"0124e204b95deda7edcf98649635eae97cfd7c44"}},{"cell_type":"code","source":"# kernel model\nKRR = KernelRidge(alpha=5)","metadata":{"_uuid":"1c9aca732e0d1637b1ede6f6b1c36684e2712bc7","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# fitting on train\nKRR.fit(X_train,y_train)","metadata":{"_uuid":"cace4899daaddc1eb53d59df6a009e33202a22d2"},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"#  model estimatores\nprint('Validation R^2: %.5f'  % r2_score(y_val,KRR.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,KRR.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,KRR.predict(X_val)))","metadata":{"_uuid":"56b947661958703682503a382e68bfbf22fdd387","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# CV error\nrmse_cv(KRR).mean()","metadata":{"_uuid":"5f8900010f789bb849eaa2a29b7e645c1e1ac1fd"},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**LASSO WITH CV**</span>","metadata":{"_uuid":"4784edbb8b26b5c92815eceb9527b3850236c129"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Lasso linear model with iterative fitting along a regularization path. The best model is selected by cross-validation.</span>","metadata":{"_uuid":"82a204c2d2ad4143d8d01b216bd77994e70e38f5"}},{"cell_type":"code","source":"# define the model with alpha values in a numpy array with cross-validation function included\nlasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005],cv =custom_cv)","metadata":{"_uuid":"ee18e20e16e9f2bcce5f403f4841d6d7123ff259","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# fit the model\nlasso.fit(X_train,y_train)","metadata":{"_uuid":"b3daf644dbd5ee9e7e06f97b8f8187540483f4e3"},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# Model scoring\nprint('Validation R^2: %.5f'  % r2_score(y_val,lasso.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,lasso.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,lasso.predict(X_val)))","metadata":{"_uuid":"b48bf4f70b84142bc512b15f669dd4da6c0ca9be","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"#CV error\nrmse_cv(lasso).mean()","metadata":{"_uuid":"a5c2ab98fa24df606dca84bd093e5931515291fc"},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**ELASTIC NET WITH CV**</span>","metadata":{"_uuid":"6d2aa54494abfe9c9a0a9c35063548d81d3002b5"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Elastic Net model with iterative fitting along regularization path. The best model is selected by cross-validation.</span>","metadata":{"_uuid":"be76292f67c42bc0da07f4c51153516fa5db7515"}},{"cell_type":"code","source":"# defining the model with alphas and Elasticnet mixing parameter l1_ratio\nElastic = ElasticNetCV(alphas=[0.0001, 0.0005, 0.001, 0.01, 0.1, 1, 10], l1_ratio=[.01, .1, .5, .9, .99], max_iter=5000\n                       ,cv=custom_cv)","metadata":{"_uuid":"e59198e15c34b80b0c3e05cf04d34ac5ce6ff01e","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"# fit the model\nElastic.fit(X_train,y_train)","metadata":{"_uuid":"4e7e2f6e47e535853d32c922babdb010373e3f1d"},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# Model scores\nprint('Validation R^2: %.5f'  % r2_score(y_val,Elastic.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,Elastic.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,Elastic.predict(X_val)))","metadata":{"scrolled":true,"_uuid":"442e86ce10a0187595f7deec4e25de85781f1a2a","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"# CV error\nrmse_cv(Elastic).mean()","metadata":{"_uuid":"d5b186ab6b7eb6599b75185ed09da33897cdf3f8"},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### PIPELINE ","metadata":{"_uuid":"dcff5398cf5acc26fd1254aedccfe36fd18d0515"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**Pipeline with GridSearchCV**</span>","metadata":{"_uuid":"53456de47047f3b30e5a70af485693e22276ee8f"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Pipeline of transforms with a final estimator.Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be ‘transforms’, that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using memory argument.</span>\n\n<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters.GridSearch CV is an Exhaustive search over specified parameter values for an estimator with a built in Cross Validation.</span>","metadata":{"_uuid":"032870c2719b591061bc164d4e32eadb5ac834ee"}},{"cell_type":"code","source":"# import pipeline\nfrom sklearn.pipeline import Pipeline\n# initialize pipeline with the model and here we have function which takes cares of outlier\nGBRPipe = Pipeline([\n        ('outlier', RobustScaler()),\n        ('gbm', GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5))])","metadata":{"_uuid":"e2eb4f5c9a10b6b905bbdf8b26c3454a35c648c3","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# set up the parameter grid with parameters you like to iterate on. Here we have 3\nparam_grid = {'gbm__learning_rate': [0.01, 0.05],\n             'gbm__max_depth': [1,5],\n             'gbm__min_samples_leaf': [10, 15]}\n             ","metadata":{"_uuid":"2d96433ceec3ff0712ca8505902f876a735f6fc0","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"# setup gridsearchcv with estimator, param_grid\nGBRGrid = GridSearchCV(estimator = GBRPipe,param_grid=param_grid)","metadata":{"_uuid":"79308cb5097242d318ec547d867205c584eb3c02","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"GBRGrid.fit(X_train,y_train)","metadata":{"_uuid":"1da96359bcc987fb7169fb4ea5da7cc0be2bae96"},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"rmse_cv(GBRGrid).mean()","metadata":{"_uuid":"104aae4a0ae529969d9af809fd60c9616a77ae48"},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"print('Validation R^2: %.5f'  % r2_score(y_val,GBRGrid.predict(X_val)))\nprint('Validation RMSE: %.5f\\n'   % mean_squared_error(y_val,GBRGrid.predict(X_val)))\nprint('Validation Mean Absolute Error: %.5f\\n'   % mean_absolute_error(y_val,GBRGrid.predict(X_val)))","metadata":{"_uuid":"531451246f09c250ef295a0be353b2dcb94ca62f","collapsed":true,"jupyter":{"outputs_hidden":true}},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"### MODEL VALIDATION","metadata":{"_uuid":"1d5565e8bf000a72a3ea7019cf5d893e6b64de27"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\"> We use the below three validators to pick our best\nmodel and in addition to that we have built a predictor function to determine the Cross Validation Error using neg_mean_squared_error scoring which also is one of our important evaluators.</span>","metadata":{"_uuid":"9a85639077a5da85eba27ecc8476c6beebbba9b8"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**R-SQUARED**</span>","metadata":{"_uuid":"54aec056645829e71c3fd827b7a52f4f4ec23b78"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the coefficient of determination, or the coefficient of multiple determination for multiple regression.Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)</span>","metadata":{"_uuid":"6ee0a851016aecb22b54f69660d7c601af0b55ed"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**RMSE (ROOT MEAN SQUARED ERROR)**</span>","metadata":{"_uuid":"b3abba916364515013e22f9feb1cccd9dbe563ac"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Root Mean Square Error (RMSE) measures how much error there is between two data sets. In other words, it compares a predicted value and an observed or actual value.It’s also known as Root Mean Square Deviation and is one of the most widely used statistic</span>","metadata":{"_uuid":"dea90fcdf0751fa943d29d322ea537d2909306cb"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 16px;\">**MEAN ABSOLUTE ERROR**</span>","metadata":{"_uuid":"e1e0d49790d8725ca794e901ca8f71292883b3f1"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">Mean absolute error (MAE) is a measure of difference between two continuous variables. Here X and y are arrays of paired observations that express the same phenomenon. Examples of Y versus X include comparisons of predicted versus actual and one technique of measurement versus an alternative technique of measurement.</span>","metadata":{"_uuid":"65386862b29628577dcf8e3b048f0886f24d8506"}},{"cell_type":"markdown","source":"### BEST MODEL","metadata":{"_uuid":"4a24b2783fedadb676ed857fe08f53aadbcfcec3"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">The Best Model here is the LassoCV Regression which manages to produce a minimum cross validation error and highest R- squared and minimal RMSE and Mean Absolute Error. It performs so good that it overtakes even the Pipeline and GridSearch CV with hyperparameter function. But the point to note here which makes pipeline and Grid search so good is that it performs better with GBR when compared to GBR alone.</span>","metadata":{"_uuid":"80e23c4729b3f80ca94e8e27014e615f0f36bac2"}},{"cell_type":"markdown","source":"### FURTHER","metadata":{"_uuid":"ec2a9dd73e4944788d929058830dcebe024db786"}},{"cell_type":"markdown","source":"<span style=\"color: black; font-family: Malgun Gothic; font-size: 14px;\">We can tune the hyperparameters further to match our best model and also add Pipeline and GridSearchCV to our models so that they include more features like outlier reduction,normalization, including more parameters and doing a grid search to find the best parameter value which brings out the best model.</span>","metadata":{"_uuid":"d74d53c082f9b869b0a64607e102c516504159ce"}}]}